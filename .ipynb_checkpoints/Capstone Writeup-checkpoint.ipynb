{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "German Credit Card Fraud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data source: UCI machine learning library, found here: http://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29\n",
    "##Project purpose:\n",
    "###I am enrolled in the Data Science with Python module under the Thinkful bootcamp program. \n",
    "###The project represents a simulation of fraud detection for adding to my project portfolio. ###An open dataset allows me to practice applying machine learning algorithms through the prediction modeling process. \n",
    "##Cases to avoid:\n",
    "I am not interested in decreasing processing time like helping the algorithm converge because the code is not meant to be scaled into production. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Methodology:\n",
    "##Software: Python using packages: sklearn, pandas, numpy, ect\n",
    "##Machine learning problem: supervised learning - classification based on 20 variables\n",
    "##Algorithm: logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Import Data\n",
    "###The data comes from the UCI repository, but does not have a link for downloading the .csv file. I copied and pasted the data found from the following link into and .xlsx file. With the data in the .xlsx file, I used delimiter for separating the columns by space and typed in each column name. I decided on leaving the categorical value names in instead of replacing with the original names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'German.csv' was not found in history, as a file, url, nor in the user namespace.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/Users/scdavis6/anaconda/lib/python3.4/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mfind_user_code\u001b[0;34m(self, target, raw, py_only, skip_encoding_cookie, search_ns)\u001b[0m\n\u001b[1;32m   3363\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m                                              \u001b[0;31m# User namespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3364\u001b[0;31m             \u001b[0mcodeobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3365\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'German' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-80663ed1c937>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Import data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'load German.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgerman_credit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'German.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/scdavis6/anaconda/lib/python3.4/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mmagic\u001b[0;34m(self, arg_s)\u001b[0m\n\u001b[1;32m   2334\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2335\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2336\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2338\u001b[0m     \u001b[0;31m#-------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/scdavis6/anaconda/lib/python3.4/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line)\u001b[0m\n\u001b[1;32m   2255\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2256\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2257\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2258\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-45>\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, arg_s)\u001b[0m\n",
      "\u001b[0;32m/Users/scdavis6/anaconda/lib/python3.4/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/scdavis6/anaconda/lib/python3.4/site-packages/IPython/core/magics/code.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, arg_s)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0msearch_ns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'n'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0mcontents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_user_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch_ns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msearch_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m's'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/scdavis6/anaconda/lib/python3.4/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mfind_user_code\u001b[0;34m(self, target, raw, py_only, skip_encoding_cookie, search_ns)\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3366\u001b[0m             raise ValueError((\"'%s' was not found in history, as a file, url, \"\n\u001b[0;32m-> 3367\u001b[0;31m                                 \"nor in the user namespace.\") % target)\n\u001b[0m\u001b[1;32m   3368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3369\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcodeobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'German.csv' was not found in history, as a file, url, nor in the user namespace."
     ]
    }
   ],
   "source": [
    "#Import data\n",
    "%load German.csv\n",
    "import pandas as pd\n",
    "german_credit = pd.read_csv('German.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dimension of dataset\n",
    "german_credit.shape\n",
    "#(1000, 21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1000 rows by 21 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Column datatypes\n",
    "german_credit.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13 categorical values and 7 numeric (excluding the class label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Number of NaN values in each column\n",
    "german_credit.notnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No NaN values found in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Summary statistics\n",
    "german_credit.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histograms of numeric variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Duration\n",
    "plt.hist(german_credit['Duration'])\n",
    "plt.xlabel('Duration')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Duration in Months')\n",
    "plt.show()\n",
    "#Credit amount\n",
    "plt.hist(german_credit['Credit amount'])\n",
    "plt.xlabel('Credit amount')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Credit Amount')\n",
    "plt.show()\n",
    "#Installment rate\n",
    "plt.hist(german_credit['Installment rate'])\n",
    "plt.xlabel('Installment rate')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Installment Rate in % of Disposable Income')\n",
    "plt.show()\n",
    "#Years of present residence - fix \n",
    "plt.hist(german_credit['Present resident since'])\n",
    "plt.xlabel('Present resident since')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Present Residence Since')\n",
    "plt.show()\n",
    "#Age\n",
    "plt.hist(german_credit['Age'])\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Age')\n",
    "plt.show()\n",
    "#Number of existing credits - fix \n",
    "plt.hist(german_credit['Number existing credits'])\n",
    "plt.xlabel('Number existing credits')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram Of Number Of Existing Credits At This Bank')\n",
    "plt.show()\n",
    "#Number of people liable\n",
    "plt.hist(german_credit['Number of people liable'])\n",
    "plt.xlabel('Number of people liable')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Number of people being liable to provide maintenance for')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count number of factors in each categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Status checking\n",
    "german_credit['Status checking'].value_counts()\n",
    "#Credit history\n",
    "german_credit['Credit history'].value_counts()\n",
    "#Purpose\n",
    "german_credit['Purpose'].value_counts()\n",
    "#Savings account/bonds\n",
    "german_credit['Savings account/bonds'].value_counts()\n",
    "#Present employment\n",
    "german_credit['Present employment'].value_counts()\n",
    "#Personal status/sex\n",
    "german_credit['Personal status/sex'].value_counts()\n",
    "#Debtors/guarantors\n",
    "german_credit['Debtors/guarantors'].value_counts()\n",
    "#Property\n",
    "german_credit['Property'].value_counts()\n",
    "#Other installment plans\n",
    "german_credit['Other installment plans'].value_counts()\n",
    "#Housing\n",
    "german_credit['Housing'].value_counts()\n",
    "#Job\n",
    "german_credit['Job'].value_counts()\n",
    "#Telephone\n",
    "german_credit['Telephone'].value_counts()\n",
    "#Foreign worker\n",
    "german_credit['Foreign worker'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check balance of class labels for good or bad creditor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "german_credit['Classification'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is unbalanced because there are more observations of the good creditors than bad. There are 700 observations of customers with good credit scores and 300 with bad. 1 represents the primary class because of the greater observations and 2 as the minority."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change classification label from (1,2) to (0,1) for aligning with sklearn's documentation. Without doing this, I got higher precision and recall scores after using the logit model. Sklearn’s precision defines identifying 1 correctly as the TP and since 1 is not the class we should predict, got a higher score. The incorrect class label resulted in predicting the occurrence of a good crediter that was 70% of the data instead of the bad creditor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "german_credit['Classification'] = german_credit['Classification'].map(lambda x: x-1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert categorical variables to dummy\n",
    "\n",
    "Logistic regression in sklearn does not automatically convert the categorical variables to binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dummy_var = pd.get_dummies(german_credit[['Status checking', 'Credit history', 'Purpose', \n",
    "                              'Savings account/bonds', 'Present employment', \n",
    "                              'Personal status/sex', 'Debtors/guarantors', \n",
    "                              'Property', 'Other installment plans', 'Housing', \n",
    "                              'Job', 'Telephone', 'Foreign worker']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify number of new columns after converting to dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dummy_var.shape   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the 54 factors turned into a binary variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset numeric variables by dropping categorical from dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "credit_new = german_credit.drop(['Status checking', 'Credit history', 'Purpose', \n",
    "                              'Savings account/bonds', 'Present employment', \n",
    "                              'Personal status/sex', 'Debtors/guarantors', \n",
    "                              'Property', 'Other installment plans', 'Housing', \n",
    "                              'Job', 'Telephone', 'Foreign worker'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get dimensions of numeric dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "credit_new.shape                              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine numeric and categorical dataframes using .join() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "german_new_credit = dummy_var.join(credit_new)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get demensions of new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "german_new_credit.shape        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn requires the logistic regression fit with a matrix or array. Separate the data into independent (X) and dependent (Y) variables, then convert to matrix format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Subset dataframe into indepedent and dependent variables\n",
    "X = german_new_credit.drop('Classification', axis = 1)\n",
    "Y = german_credit['Classification']\n",
    "#Convert dataframe to matrix \n",
    "X = X.as_matrix()\n",
    "Y = Y.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create logistic regression object\n",
    "logreg = ln.LogisticRegression()\n",
    "#Fit the logistic regression \n",
    "logreg.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Accuracy of test set\n",
    "score = cross_val_score(logreg, X, Y, scoring = 'accuracy', cv = 10)\n",
    "np.mean(score)\n",
    "#Recall of test set\n",
    "recall_score = cross_val_score(logreg, X, Y, scoring = 'recall', cv = 10)\n",
    "np.mean(recall_score)\n",
    "#Precision of test set\n",
    "precision_score = cross_val_score(logreg, X, Y, scoring = 'precision', cv = 10)\n",
    "np.mean(precision_score)\n",
    "#AUC\n",
    "auc_score = cross_val_score(logreg, X, Y, scoring = 'roc_auc', cv = 10)\n",
    "np.mean(auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model building pt.2 - Standardizing numeric variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are seven numeric variables in the dataframe and wanted to check if standardizing increases accuracy or precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset the numeric and categorical data like before. The StandardScaler() function requires the datatype as float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Subset numeric data\n",
    "num_credit = german_credit[['Duration', 'Credit amount', \n",
    "                            'Installment rate', 'Present resident since',\n",
    "                            'Age', 'Number existing credits', 'Number of people liable']]\n",
    "#Apply function to change datatype\n",
    "num_credit_st = num_credit.astype('float')                            \n",
    "#Standardization object and fit to data\n",
    "stan = StandardScaler().fit(num_credit_st)\n",
    "#Transform dataset\n",
    "stan_data = stan.transform(num_credit_st)\n",
    "#Convert array to  dataframe\n",
    "#Get strings of numeric column name_\n",
    "col_names = ['Duration', 'Credit amount', 'Installment rate', 'Present resident since',\n",
    "             'Age', 'Number existing credits', 'Number of people liable']\n",
    "new_stan = pd.DataFrame(stan_data, columns = col_names)                            \n",
    "\n",
    "\n",
    "#Subset categorical data\n",
    "cat_credit = german_credit[['Status checking', 'Credit history', 'Purpose', \n",
    "                              'Savings account/bonds', 'Present employment', \n",
    "                              'Personal status/sex', 'Debtors/guarantors', \n",
    "                              'Property', 'Other installment plans', 'Housing', \n",
    "                              'Job', 'Telephone', 'Foreign worker']]\n",
    "#Change categorical variables to dummy\n",
    "dummy_var = pd.get_dummies(cat_credit)  \n",
    "#Join dataframes together\n",
    "german_new_credit = dummy_var.join(new_stan) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model building and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Subset dataframe into indepedent and dependent variables\n",
    "X_st = german_new_credit\n",
    "Y_st = german_credit['Classification'] \n",
    "\n",
    "#Create logistic regression object\n",
    "logreg_st = ln.LogisticRegression()\n",
    "#Convert dataframe to matrix \n",
    "X_stm = X_st.as_matrix()\n",
    "Y_stm = Y_st.as_matrix()\n",
    "\n",
    "#Fit the logistic regression \n",
    "logreg_st.fit(X_stm, Y_stm)\n",
    "\n",
    "#Accuracy of test set\n",
    "score_st = cross_val_score(logreg_st, X_stm, Y_stm, scoring = 'accuracy', cv = 10)\n",
    "np.mean(score_st)\n",
    "#Recall of test set\n",
    "recall_score_st = cross_val_score(logreg_st, X_stm, Y_stm, scoring = 'recall', cv = 10)\n",
    "np.mean(recall_score_st)\n",
    "#Precision of test set\n",
    "precision_score_st = cross_val_score(logreg_st, X_stm, Y_stm, scoring = 'precision', cv = 10)\n",
    "np.mean(precision_score_st)\n",
    "#AUC\n",
    "auc_score_st = cross_val_score(logreg_st, X_stm, Y_stm, scoring = 'roc_auc', cv = 10)\n",
    "np.mean(auc_score_st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Say how standardization did not help the analysis by more than 1000th of a percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model building pt. 3 - fixing unbalanced dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing from pt.1 of combining the categorical and numeric data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UnbalancedDataset package is for python and provides different algorithms for fixing unbalanced datastets. Here’s the link: https://github.com/fmfn/UnbalancedDataset. I applied the SMOTE algorithm to create 400 more observations of variables with the minority class (bad creditors) for a 50/50 distribution of 700 each. I weighted the proportions of the majority to minority class for a 50/50 ratio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Ratio of majority to minority class for 50/50 distribution\n",
    "smote = SMOTE(ratio = 1.335, verbose = False, kind = 'regular')\n",
    "#Fit data and transform\n",
    "X_mod = X.as_matrix()\n",
    "Y_mod = np.array(Y)\n",
    "#Create new dataset\n",
    "smox, smoy = smote.fit_transform(X_mod, Y_mod) \n",
    "#Check ratio of good and bad creditors\n",
    "#Convert matrix to dataframe\n",
    "y_data = pd.DataFrame(smoy, columns = ['classification'])\n",
    "#check work\n",
    "y_data['classification'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model building and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create logistic regression object\n",
    "logreg_ov = ln.LogisticRegression()\n",
    "#Model building\n",
    "#Fit the logistic regression \n",
    "logreg_ov.fit(smox, smoy)\n",
    "\n",
    "#Model testing\n",
    "#Accuracy of test set\n",
    "score_ov = cross_val_score(logreg_ov, smox, smoy, scoring = 'accuracy', cv = 10)\n",
    "np.mean(score_ov)\n",
    "#Recall of test set\n",
    "recall_score_ov = cross_val_score(logreg_ov, smox, smoy, scoring = 'recall', cv = 10)\n",
    "np.mean(recall_score_ov)\n",
    "#Precision of test set\n",
    "precision_score_ov = cross_val_score(logreg_ov, smox, smoy, scoring = 'precision', cv = 10)\n",
    "np.mean(precision_score_ov)\n",
    "#AUC\n",
    "auc_score_ov = cross_val_score(logreg_ov, smox, smoy, scoring = 'roc_auc', cv = 10)\n",
    "np.mean(auc_score_ov)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
